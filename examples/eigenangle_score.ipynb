{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical test of eigenvector angles to evaluate the similarity of neural network simulations\n",
    "### Or how the analytic description of random angles and eigenvalues can help to compare matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "from quantities import Hz, ms\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Dropdown, fixed\n",
    "\n",
    "from elephant.spike_train_correlation import corrcoef\n",
    "from elephant.conversion import BinnedSpikeTrain\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process as HPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [An angle measure to evaluate similiarity between matrices](#intro)\n",
    "    * [The context](#intro-a)\n",
    "    * [The problem](#intro-b)\n",
    "    * [The idea](#intro-c)\n",
    "    * [The outline](#intro-d)\n",
    "* [On the behavior of angles](#angles)\n",
    "    * [Angles in high dimensions](#angles-a)\n",
    "    * [Simulating random vectors](#angles-b)\n",
    "    * [Simulating random eigenvectors](#angles-b)\n",
    "* [Angles as similarity measure](#smallness)\n",
    "    * [Angle-smallness](#smallness-a)\n",
    "    * [Distribution of eigenvalues](#smallness-b)\n",
    "    * [Weighted angle-smallness](#smallness-c)\n",
    "* [Building a statistical test](#test)\n",
    "    * [Similarity score](#test-a)\n",
    "    * [Null hypothesis](#test-b)\n",
    "    * [P-value and interpretation](#test-c)\n",
    "* [Application to neural activity data](#apply)\n",
    "    * [Practical considerations](#apply-a)    \n",
    "    * [Example data](#apply-b)\n",
    "    * [Parameter scan](#apply-c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# An angle measure to evaluate similiarity between matrices\n",
    "\n",
    "<a id='intro-a'></a>\n",
    "## The context\n",
    "There are many ways to compare activity data of neural networks, both for data obtained from biological experiments and from model simulations. This is great, because the whole concept of model validation is based on the rigorous comparison between experimental and simulated data. A fairly straight-forward way to compare network activity data is via node-wise (i.e neuron-wise) measures, such as firing rates, spiking regularity, or graph centrality. These neuron-wise measures are statistically independent and can be represented by sample distributions which can be compared by the [wide variety of statistical two-sample tests](https://en.wikipedia.org/wiki/Two-sample_hypothesis_testing). Popular tests are for example, the Student's t-test, the Kolmogorov-Smirnov test, or the Wilcoxon rank-sum test. Even though they are not formally a statistical tests, also comparative measures such as the Kullback-Leibler divergence and the effect size have to be mentioned.\n",
    "\n",
    "<a id='intro-b'></a>\n",
    "## The problem\n",
    "However, not all measures to characterize network activity are statistically independent. Since the key feature of a network is its interconectiveness, argueably the most interesting characteristics are captured by pairwise or high-order measures like correlation, effective connectivity, or Granger causality. Such measures can as well be represented by simple sample distributions, but this would neglect the dependence between the individual values. For example, neuron A has a certain correlation with neuron B, and neuron B a correlation with neuron C, then the correlation which is possible between neuron A and C is dependent by their correlations with neuron B. So in order to not loose this dependence, the values need to be represented not as a sample distribution but as a matrix. Comparing matrices in a meaningful way is however much more tricky than comparing distributions, and few statistical tests are available.\n",
    "\n",
    "<a id='intro-c'></a>\n",
    "### The idea\n",
    "The various two-sample tests typically compare certain attributes of the distributions, e.g. the mean, the variance, or the shape. For the case of matrices we can analogously consider the eigenvalues and eigenvectors as relevant attributes. The eigenvectors span the space in which the data in the matrix is represented most naturally, in the sense that the first eigenvector points along the direction of the most variance in the data, the second along the direction of the most variance within the orthogonal subspace, and so on; whereas the corresponding eigenvalues quantify the variance along these axes. Thus, for comparison of two matrices, we could measure the similarity between them by means of the similariy of the eigenvectors, especially the first ones, i.e., the ones with the largest eigenvalues. And how to evaluate the similarity between vectors? Of course, angles!\n",
    "\n",
    "<a id='intro-d'></a>\n",
    "### The outline\n",
    "In the following we will build up a theoretical basis to evaluate the similarity between matrices by considering the angles between eigenvectors.\n",
    "First, we will describe the behavior of angles for high dimensional vectors, especially eigenvectors.\n",
    "Then, we will look the smallness of angles as a indicator for similarity and integrate the eigenvalues as a weighting factor into the analytical description.\n",
    "Based on this we formulate a similarity score for a matix comparison and describe its theoretical distribution to compute a corresponding p-value.\n",
    "Finally, we apply this statistical test to compare neural network activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='angles'></a>\n",
    "# The distribution of random angles\n",
    "\n",
    "<a id='angles-a'></a>\n",
    "## Angles in high dimensions\n",
    "The basic idea is that a small angle between two vectors indicates similarity, a large angle indicates discrepancy, where the angle is defined as $\\phi=\\arccos(\\mathbf{v}_1 \\cdot \\mathbf{v}_2)$. But in order to know when an angle is large and when it is small we need to relate it to a reference scenario, i.e. the null hypothesis. Here, the null hypothesis would be that both vectors point into a random direction, or to be precise they are random vectors uniformly distributed over the N-dim unit sphere. As the vectors exist in a 'neuron space' corresponding to the dimensionality of our matrix, N is equal to the number of neurons we record from.\n",
    "\n",
    "This may seem overly formal since we just want to distinguish small and large angles, but this is crucial because angles behave very differently depending on the vector dimensions. In two dimension the angle distribution is still excatly how we would expect them to be. Each angle between random vectors is equally likely and thus the distribution is uniform between $0$ and $\\pi$. However, once we leave the simple 2D world and move to higher dimensions things get fairly unintuitive. For example, consider ball of diameter d within a cube with edge length d. In 2 or 3 dimensions nearly all the volume of the cube is also within the ball, but in higher dimensions the volume concentrates more and more at the surface of the cube, so that it is outside of the ball. So that means, if you stand in a random spot in a high-dimensional classroom, you'll nearly always stand in a corner. Part of the same phenomenon, and relevant for our angles, is that in high dimensions random vector pairs are nearly always perpendicular.\n",
    "\n",
    "Fortunately, the probability distribution $f_\\sphericalangle(\\phi)$ of random angles depending on the dimension $N$ can be described analytically as [(*Cai et al., 2014*)]()\n",
    "$$\n",
    "f_\\sphericalangle(\\phi) =  \\frac{\\Gamma(\\frac{N}{2})}{\\sqrt{\\pi} \\Gamma(\\frac{N-1}{2})} \\sin^{N-2}(\\phi) \\qquad \\phi \\in [0,\\pi],\n",
    "$$\n",
    "or as written in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_dist(phi, N):\n",
    "    if phi >= 0 and phi <= np.pi:\n",
    "        return math.gamma(N/2.) / (np.sqrt(np.pi) \\\n",
    "             * math.gamma((N-1)/2)) * np.sin(phi)**(N-2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='angles-b'></a>\n",
    "## Angle distribution for random vectors\n",
    "\n",
    "Let's see how this looks like in practice. First, we need to generate some uniformly distributed random vectors. This is most easily done by drawing each vector component from a normal distribution and then normalizing the vectors [(*Guhr et al., 1998*)]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vectors(N, samples):\n",
    "    random_vectors = np.random.normal(size=(samples, N))\n",
    "    for i, v in enumerate(random_vectors):\n",
    "        random_vectors[i] /= np.linalg.norm(v)\n",
    "    return random_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check that this method actually produces uniformly distributed random vectors, we plot a bunch of them in a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "rand_vectors = generate_random_vectors(N=3, samples=1000)\n",
    "\n",
    "ax.scatter(rand_vectors[:,0],rand_vectors[:,1],rand_vectors[:,2], \n",
    "           color='r', s=20, label='random vectors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random vectors appear to be indeed uniformly distributed. \n",
    "The random angles we compute by sampling two sets of random vectors and instead of calculating the arcus cosine of the scalar product of each pair separately, we apply the equivalent operation of multiplying the two sets of vectors and calculating the arcus cosine on the diagonal of the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_angles(N, samples, vector_function):\n",
    "    vectors_a = vector_function(N=N, samples=samples)\n",
    "    vectors_b = vector_function(N=N, samples=samples)    \n",
    "    M = np.dot(vectors_a, vectors_b.T)\n",
    "    return np.arccos(np.diag(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to illustrate how the random angles depend on the the dimension of the vectors we plot distribution of the sampled angles along with the analytical distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fdd3c2e3a64844b19213c7d600801d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='N', max=30, min=2), IntSlider(value=1000, description='s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "def plot_angle_distribution(N, samples, vector_function, bins=20):\n",
    "    sampled_angles = generate_random_angles(N=N, samples=samples,\n",
    "                                            vector_function=vector_function)\n",
    "    \n",
    "    # compute sample distribution\n",
    "    bin_edges = np.linspace(0, np.pi, bins)\n",
    "    hist, _ = np.histogram(sampled_angles, bins=bin_edges, density=True)\n",
    "    \n",
    "    # clear plot\n",
    "    plt.cla()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # plot sample distribution\n",
    "    bin_centers = bin_edges[:-1] + np.diff(bin_edges)[0]/2.\n",
    "    ax.bar(bin_centers, hist, width=.9*np.diff(bin_centers)[0], alpha=0.5, label='sample distribution')\n",
    "    \n",
    "    # plot analytic distribution\n",
    "    phi = np.linspace(0, np.pi, 100)\n",
    "    ax.plot(phi, [angle_dist(p, N) for p in phi], 'k--', label=r'$\\sim\\sin^{N-2}\\phi$')\n",
    "\n",
    "    # format figure\n",
    "    ax.set_xlabel('angle $\\phi$')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_xticks(np.array([0, .25, .5, .75, 1]) * np.pi)\n",
    "    ax.set_xticklabels(['0', r'$\\frac{1}{4}\\pi$', r'$\\frac{1}{2}\\pi$',\n",
    "                        r'$\\frac{3}{4}\\pi$', r'$\\pi$'])\n",
    "    ax.set_title('{}D angle distribution'.format(int(N)))\n",
    "    plt.legend()\n",
    "    return None\n",
    "\n",
    "# interactive plot with variable dimension N, and number of sample angles\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "interact(plot_angle_distribution, \n",
    "         N=IntSlider(value=2, min=2, max=30, step=1),\n",
    "         samples=IntSlider(value=10**3, min=0, max=10**4, step=10**3),\n",
    "         vector_function=fixed(generate_random_vectors), bins=fixed(20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='angles-c'></a>\n",
    "## Angle distribution for random eigenvectors\n",
    "\n",
    "But wait! Originally we were interested in the angles between eigenvectors, which we know are not uniformly distributed random vectors because they need to be pairwise orthogonal by definition.\n",
    "However, this turns out to be only a problem for small dimensions, while the analytical function still very well describes the distribution of these 'eigenangles' in higher dimensions. Let's come back to this and first generate some random eigenvectors.\n",
    "\n",
    "In order to generate random eigenvectors we need to generate random matricies. Here, we want to focus on correlation matrices, the theory however is easily generalizable to other types of matrices.\n",
    "Correlation matrices are defined by the following properties.\n",
    "* The matrix is symmetric.\n",
    "* All elements are real and $\\in$ [$-1,1$].\n",
    "* The diagonal elements are $1$.\n",
    "* The matrix is positive definte, i.e., all eigenvalues are $\\ge 0$.\n",
    "\n",
    "A random correlation matrix can be created by calculating the Gram matrix from a set of normalized random vectors $\\mathbf{R} = \\mathbf{T}\\mathbf{T}^*$, where $\\mathbf{T}$ is a matrix with rows $T_i$ being noramlized random vectors [(*Homes, 1991*)](). The dimension of the row vectors $T_i$ does not influence the distribution of the eigenvectors. Therefore, we describe this degree of freedom as $\\alpha\\cdot N$ and arbitrarily choose $\\alpha=1$. This has no relevance for the eigenvectors or angle distribution, but will become important later for the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_eigendecomposition(N, samples, alpha):\n",
    "    nbr_of_matrices = int(samples//N)\n",
    "    eigenvector_array = np.empty((nbr_of_matrices*N, N))\n",
    "    eigenvalue_array = np.empty(nbr_of_matrices*N)\n",
    "    for i in range(nbr_of_matrices):\n",
    "        random_vectors = np.random.normal(size=(N, alpha*N))\n",
    "        for j, v in enumerate(random_vectors):\n",
    "            random_vectors[j] /= np.linalg.norm(v)\n",
    "        random_corr_matrix = np.dot(random_vectors, random_vectors.T)\n",
    "        eigenvalues, eigenvectors = sc.linalg.eigh(random_corr_matrix)\n",
    "        eigenvector_array[i*N:i*N+N] = eigenvectors.T\n",
    "        eigenvalue_array[i*N:i*N+N] = eigenvalues        \n",
    "    return eigenvalue_array, eigenvector_array\n",
    "\n",
    "def generate_random_eigenvectors(N, samples):\n",
    "    return generate_random_eigendecomposition(N, samples, alpha=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot demonstrates how also the angles between eigenvectors reasonably follow the analytic angle distribution when the dimension $N > 10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364bf9f6d22c4044b8dc1d1d1d9193f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='N', max=30, min=2), IntSlider(value=1000, description='s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "interact(plot_angle_distribution, \n",
    "         N=IntSlider(value=2, min=2, max=30, step=1),\n",
    "         samples=IntSlider(value=10**3, min=0, max=10**4, step=10**3),\n",
    "         vector_function={'random eigenvectors':generate_random_eigenvectors,\n",
    "                          'random vectors':generate_random_vectors}, \n",
    "         bins=fixed(20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='smallness'></a>\n",
    "# Angles as similarity measure\n",
    "\n",
    "<a id='smallness-a'></a>\n",
    "## Angle smallness\n",
    "\n",
    "In the beginning, we set out to identify matrix similarity by small angles between their eignvectors. Now that we have a formal description on how to evaluate angle size depending on the vectors' dimension, we can define a corresponding similarity measures, i.e., the *angle-smallness*. So the quantitiy we are actually interested in, is not the absolute angle size but its quantile position in the distribution of random angles. Thus, we define the auxiliary variable $\\Delta = 1 - \\frac{\\phi}{\\pi/2}$ to represent the smallness of the angle on a scale form $-1$ to $1$. By performing a simple variable transformation we see that in order to adapt the angle distribution accordingly, we just have to replace the sine function by a cosine and scale with $\\frac{\\pi}{2}$. \n",
    "$$\n",
    "\\tilde{f_\\sphericalangle}(\\Delta) \\propto \\cos^{N-2}(\\Delta\\cdot\\pi/2) \\qquad \\Delta \\in [-1, 1] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_smallness_dist(D, N):\n",
    "    if D >= -1 and D <= 1:\n",
    "        return math.gamma(N/2.) / (np.sqrt(np.pi) * math.gamma((N-1)/2)) \\\n",
    "             * np.pi/2 * np.cos(D*np.pi/2)**(N-2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another property of eigenvectors, besides being pairwise orthogonal, is that they have an inherent order. They are sorted according to the magnitude of their corresponding eigenvalue. So, the eigenvector with the largest eigenvalue is considered the *first* and argueably the most important, as it represents the axis of largest variance. Consequently, this also means that the angle between the first eigenvectors is a more relevant quantification of the matrices similarity than the angle between the last eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='smallness-b'></a>\n",
    "## Distribution of eigenvalues\n",
    "\n",
    "To incorporate this aspect into the test, we want to weight the smallness of each angle with the corresponding eigenvalues. This requires that we know how the eigenvalues are distributed, under the null hypothesis, i.e, for a random matrix.\n",
    "Fortunately again, the eigenvalue distribution is known for certain types of random matrices, at least in the limit of infinitly sized matrices. For the case of matrices (which includes correlation matrices) of type $\\mathbf{Y}_{N}=\\mathbf{XX}^{T}$ where $\\mathbf{X}$ is a $(\\alpha N)\\times N$ random matrix whose entries are independent identically distributed random variables with mean $0$ and variance $\\sigma^{2}<\\infty$, the asymptotic eigenvalue distribution was described by [Vladimir Marcenko and Leonid Pastur in 1967](https://doi.org/10.1070/SM1967v001n04ABEH001994).\n",
    "This Marchenko-Pastur distribution is only depended on the parameter $\\alpha$, which we already introduced as the ratio between the length of the row vectors $T_i$ and the dimensionality $N$. Naturally, the distribution can't depend on $N$ as it assumes an $N \\rightarrow \\infty$. It is defined as $h_{\\alpha}(\\lambda)$,\n",
    "\n",
    "$$\n",
    "h_{\\alpha}(\\lambda) = \\frac{\\alpha}{2 \\pi \\lambda} \\sqrt{(\\lambda_+ - \\lambda) \\cdot (\\lambda - \\lambda_-)}\n",
    "\\\\\n",
    "\\lambda_{\\pm} = \\left(1 \\pm \\sqrt{\\frac{1}{\\alpha}}\\right)^2 ,\n",
    "$$\n",
    "\n",
    "or as written in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marchenko_pastur(x, alpha):\n",
    "    assert alpha >= 1\n",
    "    x_min = (1 - np.sqrt(1. / alpha)) ** 2\n",
    "    x_max = (1 + np.sqrt(1. / alpha)) ** 2  \n",
    "    y = alpha / (2 * np.pi * x) * np.sqrt((x_max - x) * (x - x_min))\n",
    "    if np.isnan(y):\n",
    "        return 0\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the distribution and its fit to sampled eigenvalues, we again generate random matricies with varying $N$ and $\\alpha$. \n",
    "And as we can easily observe, the dimension $N$ indeed does not influence the shape eigenvalue distribution. The only effect of increasing $N$ is that the sample variance decreases so that the histogram fits the analytic distribution more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fe181ae1af47c6a46ee07502602967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac04e4caa8b4060aade45733b7d8b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='N', max=200, min=2), IntSlider(value=10, description='a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "def plot_eigenvalue_distribution(N, alpha, samples, bins=20):\n",
    "    sampled_eigenvalues, _ = generate_random_eigendecomposition(N=N, \n",
    "                                                                alpha=alpha, \n",
    "                                                                samples=samples) \n",
    "    \n",
    "    # compute sample distribution\n",
    "    hist, edges = np.histogram(sampled_eigenvalues, bins=bins, density=True)\n",
    "    \n",
    "    # clear plot\n",
    "    plt.cla()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # plot sample distribution\n",
    "    bin_centers = edges[:-1] + np.diff(edges)[0]/2\n",
    "    ax.bar(bin_centers, hist, width=.9*np.diff(bin_centers)[0], alpha=0.5,\n",
    "           label='sample distribution')\n",
    "    \n",
    "    # plot analytic distribution\n",
    "    xvalues = np.linspace(edges[0], edges[-1], 100)\n",
    "    ax.plot(xvalues, [marchenko_pastur(x, alpha) for x in xvalues], 'k--',\n",
    "            label='Marchenko-Pastur distribution')\n",
    "    \n",
    "    # format plot\n",
    "    ax.set_xlabel('eigenvalue $\\lambda$')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_title(r'Eigenvalue distribution ($\\alpha = $ {})'.format(alpha))\n",
    "    plt.legend()\n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "interact(plot_eigenvalue_distribution, \n",
    "         alpha=IntSlider(value=10, min=1, max=500, step=1),\n",
    "         N=IntSlider(value=10, min=2, max=200, step=1),\n",
    "         samples=IntSlider(value=1000, min=0, max=2000, step=100),\n",
    "         bins=fixed(20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='smallness-c'></a>\n",
    "## Weighted angle-smallness\n",
    "\n",
    "As every angle involves two eigenvectors, we have to combine two eigenvalues for a corrsponding weight. Thus, for the two matrices $\\mathbf{A}$ and $\\mathbf{B}$ with the eigenvectors $\\mathbf{v}_{A,i}$ and $\\mathbf{v}_{B,i}$ and eigenvalues $\\lambda_{A,i}$ and $\\lambda_{B,i}$, we define the weight for the angle-smallness $\\Delta$ as the root mean square of the eigenvalues: $w_i \\propto \\sqrt{(\\lambda_{A,i}^2 + \\lambda_{B,i}^2)/2}$ with $\\sum w_i = N$. By choosing the weights like this, we ensure that distribution of the weights also follows the Marchenko-Pastur distribution $h_{\\alpha}$. So, under the null hypothesis of two random matrices we can formulate the distribution of both $\\Delta\\phi_i$ and the weights $w_i$. To get the distribution of $w_i \\cdot \\Delta_i$, we combine the two distribution with the following expression $g_{N,\\alpha}$.\n",
    "\n",
    "$$\n",
    "g_{N,\\alpha}(w\\Delta) = \\int_{\\lambda_-}^{\\lambda_+} \\tilde{f_N}(\\frac{\\Delta}{\\lambda}) \\cdot h_{\\alpha}(\\lambda) \\cdot \\frac{d\\lambda}{\\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_smallness_dist(D, N, alpha): \n",
    "    x_min = (1 - np.sqrt(1. / alpha)) ** 2\n",
    "    x_max = (1 + np.sqrt(1. / alpha)) ** 2 \n",
    "    \n",
    "    integrand = lambda x, _D, _N, _alpha: angle_smallness_dist(_D / float(x), _N) \\\n",
    "                                        * marchenko_pastur(x, _alpha) \\\n",
    "                                        * 1. / x\n",
    "    return sc.integrate.quad(integrand, x_min, x_max, args=(D,N,alpha,))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how this combined distribution behaves in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d519e7f9ca194626bcf7c9dcc60dee75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14097c4480d54ba6af132571714c15eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='N', max=200, min=2), IntSlider(value=10, description='a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_weighted_smallness_distribution(N, alpha, samples, bins=20):\n",
    "    # generate pairs of random eigenvectors and values\n",
    "    eigenvalues_A, eigenvectors_A = generate_random_eigendecomposition(N=N, \n",
    "                                                                       samples=samples, \n",
    "                                                                       alpha=alpha)\n",
    "\n",
    "    eigenvalues_B, eigenvectors_B = generate_random_eigendecomposition(N=N, \n",
    "                                                                       samples=samples, \n",
    "                                                                       alpha=alpha)\n",
    "    # calculate eigenangles and their smallness\n",
    "    M = np.dot(eigenvectors_A, eigenvectors_B.T)\n",
    "    angles = np.arccos(np.diag(M))\n",
    "    smallness = 1 - angles / (np.pi/2.)\n",
    "\n",
    "    # calculate weights and apply to smallness\n",
    "    weights = np.sqrt((eigenvalues_A ** 2 + eigenvalues_B ** 2) / 2.)\n",
    "    weights = weights / sum(weights) * N\n",
    "    weighted_smallness = smallness * weights \n",
    "    weighted_smallness = weighted_smallness * len(smallness)/N # correct for multiple matrices\n",
    "    \n",
    "    # compute sample distribution\n",
    "    bin_edges = np.linspace(-1, 1, bins)\n",
    "    hist, _ = np.histogram(weighted_smallness, bins=bin_edges, density=True)\n",
    "    \n",
    "    # clear plot\n",
    "    plt.cla()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # plot sample distribution\n",
    "    bin_centers = bin_edges[:-1] + np.diff(bin_edges)[0]/2.\n",
    "    ax.bar(bin_centers, hist, width=.9*np.diff(bin_centers)[0], alpha=0.5, \n",
    "           label='sample distribution')\n",
    "    \n",
    "    # plot the analytical distribution\n",
    "    xvalues = np.linspace(-1, 1, 100)\n",
    "    ax.plot(xvalues, [weighted_smallness_dist(x, N, alpha) for x in xvalues], 'k--',\n",
    "            label=r'analytical distribution $g_{N,\\alpha}$')\n",
    "    \n",
    "    # format plot\n",
    "    ax.set_xlabel(r'weighted angle-smallness $w\\cdot\\Delta$')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.set_title(r'weighted angle-smallness ({}D, $\\alpha=${})'.format(N, alpha))\n",
    "    plt.legend()\n",
    "    return None\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "interact(plot_weighted_smallness_distribution, \n",
    "         alpha=IntSlider(value=10, min=1, max=500, step=1),\n",
    "         N=IntSlider(value=10, min=2, max=200, step=1),\n",
    "         samples=IntSlider(value=1000, min=0, max=2000, step=100),\n",
    "         bins=fixed(20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "# Building the statistical test\n",
    "\n",
    "<a id='test-a'></a>\n",
    "## Similarity score\n",
    "\n",
    "We use the quantity of the weighted angle-smallness as a measure for the matrix similarity, but to evaluate the similarity of two $N\\times N$ matrices it is not very handy to have $N$ different values. Thus, we average over the $N$ to arrive at a single valued similariy score.\n",
    "$$\n",
    "\\eta = \\frac{1}{N} \\sum_i^N w_i \\cdot \\Delta_i\n",
    "$$\n",
    "A large $\\eta$ indicates that on average the weighted angles between the eigenvectors are smaller than would be expected for random matrices, and this we initialy wanted to measure as similarity. \n",
    "\n",
    "In order to interpret a given $\\eta$ for a sample with a certain $N$ and $\\alpha$, we also need to know how $\\eta$ is theoretically distributed. Since we know how $w \\Delta \\phi$ is distributed and $\\eta$ is the average of $N$ samples of $w \\Delta \\phi$, for a sufficiently large $N$ we can apply the central limit theorm.\n",
    "\n",
    "The central limit theorem states that the mean of a set of idependent sample values (here $w_i\\Delta_i$) tends to be normally distributed, regardless of the distribution shape for the sample values. This normal distribution centers around the expected mean (here $0$) with the standard deviation $\\sigma = \\frac{s}{\\sqrt{N}}$, where $s$ is the standard deviation of $g_{N,\\alpha}(w\\Delta)$. Following the definition for the standard deviation for continous distributions, we can calculate $s$ as the integral over the product of the distribution with the squared distance to the mean. \n",
    "\n",
    "So, finally we arrive at an analytical description of the similarity score distribution $f(\\eta)$.\n",
    "\n",
    "$$\n",
    "f(\\eta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{\\eta^2}{2\\sigma^2})\\\\\n",
    "\\sigma^2=  \\frac{1}{N} \\int x^2 \\cdot g_{N,\\alpha}(x) \\ dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_distribution(eta, N, alpha):\n",
    "    integrand = lambda x, N_, alpha_: x**2 * weighted_smallness_dist(x, N_, alpha_)\n",
    "    var = sc.integrate.quad(integrand, -np.infty, np.infty, args=(N,alpha,))[0]\n",
    "    sigma = np.sqrt(var/N)\n",
    "    return sc.stats.norm.pdf(eta, 0, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a final check we plot a sample histogram of similarity scores alongside the theoretical distribution. In contrast to the previous plots, the number of samples in the histogram is no longer the number of vector pairs or eigenvalues, but the number of matrix pairs. Thus, to avoid confusion we call them *runs*. \n",
    "\n",
    "Because, each matrix pair generates only one similarity score, contrasting to $N$ eigenangles, it takes considerably longer to sample a reasonable amout to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_similarity_scores(N, alpha, runs):\n",
    "    similarity_scores = np.zeros(runs)\n",
    "    \n",
    "    samples = N\n",
    "\n",
    "    for run in range(runs): \n",
    "        # generate pairs of random eigenvectors and values\n",
    "        eigenvalues_A, eigenvectors_A = generate_random_eigendecomposition(N=N, \n",
    "                                                                           samples=samples, \n",
    "                                                                           alpha=alpha)\n",
    "\n",
    "        eigenvalues_B, eigenvectors_B = generate_random_eigendecomposition(N=N, \n",
    "                                                                           samples=samples, \n",
    "                                                                           alpha=alpha)\n",
    "        M = np.dot(eigenvectors_A, eigenvectors_B.T)\n",
    "        angles = np.arccos(np.diag(M))\n",
    "        smallness = 1 - angles / (np.pi/2.)\n",
    "\n",
    "        # calculate weights and apply to smallness\n",
    "        weights = np.sqrt((eigenvalues_A ** 2 + eigenvalues_B ** 2) / 2.)\n",
    "        weights = weights / sum(weights) * N\n",
    "        weighted_smallness = smallness * weights\n",
    "\n",
    "        similarity_scores[run] = np.mean(weighted_smallness)\n",
    "    \n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "alpha = 500\n",
    "runs = 1000\n",
    "\n",
    "similarity_scores = sample_similarity_scores(N, alpha, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86d2ab77f0a44b49f4c8ec874bafc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = 20\n",
    "xlims = (-.25, .25)\n",
    "\n",
    "# compute sample distribution\n",
    "bin_edges = np.linspace(xlims[0], xlims[1], bins)\n",
    "hist, _ = np.histogram(similarity_scores, bins=bin_edges, density=True)\n",
    "\n",
    "# plot sample distribution\n",
    "bin_centers = bin_edges[:-1] + np.diff(bin_edges)[0]/2.\n",
    "ax.bar(bin_centers, hist, width=.9*np.diff(bin_centers)[0], alpha=0.5, \n",
    "       label='sample distribution')\n",
    "\n",
    "# plot the analytical distribution\n",
    "xvalues = np.linspace(xlims[0], xlims[1], 100)\n",
    "ax.plot(xvalues, similarity_score_distribution(xvalues, N, alpha), 'k--',\n",
    "        label=r'analytical distribution $f(\\eta)$')\n",
    "\n",
    "# format plot\n",
    "ax.set_xlabel(r'similarity scores $\\eta$')\n",
    "ax.set_ylabel('density')\n",
    "ax.set_title(r'Null distribution of eigenangle test ({}D, $\\alpha=${})'.format(N, alpha))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test-b'></a>\n",
    "## Null hypothesis\n",
    "The basis of the null hypothesis is that the two matrices are independent, so that the angles between their eigenvectors can be described by a random angle distribution.\n",
    "\n",
    "Furthermore we assumed that $N$ is large, so that the constraint that eigenvectors are orthogonal could be neglected for the angle distribution, and so that we could apply the central limit theorem in the last step. We assumed the eigenvalues follow a Marchenko-Pastur distribution, which requires as well an $N \\rightarrow \\infty$ and the matrices to be of the type $\\mathbf{Y}_{N}=\\mathbf{XX}^{T}$, as described in the section [*Distribution of eigenangles*](#smallness-b). With the numerical simulations along the way we found that $N>10$ is already sufficient so that by visual inspection the null distribution $f(\\eta)$ reasonably represents the randomly sampled test data.\n",
    "\n",
    "The working principle of a Null Hypothesis Significance Test (NHST) is that any violations of the null hypothesis in real data results in a score which lies more in the margins of the distribution, and is therefore less likely to be explained by stochastic variation. However, there are many different ways how the null hypothesis could be violated. For example, $N$ could be small, the matrices or subsets of the matrices could be correlated or anti-correlated, or the matrices slightly deviate from being symmetric. There is no reason to assume that the test is equally susceptive to the  different kinds of deviations from the null hypothesis. Without extensive calibration it therefore is very hard to say exactly how well the test detects similarity, and what this similarity means in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test-c'></a>\n",
    "## P-value and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the null hypothesis is fullfiled for a dataset, the resulting similarity score will be a random variable with the probability distribution $f(\\eta)$. Here, the distribution is centered around $0$, so $0$ would be the most likely value to expect. The idea is that when the null hypothesis is not applicable anymore and the matrices are correlated, the similarity score will be larger. Ideally, a substantial correlation between the matrices results in a score which is very unlikely to be explained just by the stochastic spread of the null distribution.\n",
    "To quantify this evaluation, the probability of a score-value to be a random variable of the null distribution is defined as the integral over the quantile of distribution the value falls into.\n",
    "$$\n",
    "p_{\\eta} = 2 \\int_{|\\eta|}^{\\infty} f(x) \\ dx\n",
    "$$\n",
    "This probability value is commonly refered to as the $p$-value* of the statistical test, or more precisely the two-sided $p$ value.\n",
    "However, here we are only interested in scores which deviate two larger values, i.e. the right hand side of the distribution. Thus, our test is a one-sided test. Deviation to large negative values are also unlikely to be explained by the null distribution, but do not indicate similarity. On the contrary, in that scenario the corrsponding eigenvectors of the two matrices tend to be perpendicular, which could be interpreted as an anti-correlation. So in order to think of the $p$ value as the probability of non-similarity, we use the one-sided $p$ value\n",
    "$$\n",
    "p_{\\eta} = \\int_{\\eta}^{\\infty} f(x) \\ dx.\n",
    "$$\n",
    "This means as well that the average score $\\eta=0$ corresponds to a $p$ value of $0.5$, and scores with large negative values have $p$ values $\\rightarrow 1$. So as with every NHST we do not test the property of interest (here similarity) directly, but conclude it as an alternative hypothesis when the null hypothesis (here independence) is rejected due to a small $p$ value.\n",
    "\n",
    "Note: Historically, the decision whether or not to reject the null hypothesis is often done by setting a rather arbitrary significance level (e.g. $0.05$) and considering the $p$ value 'significant' when it's smaller and therefore reject the null hypothesis. There are various problems with that which are discussed in detail in various articles and papers, for example [here](https://www.nature.com/articles/d41586-019-00857-9), [here](https://doi.org/10.3389/fnhum.2017.00390), and [here](https://doi.org/10.1111/j.1469-185X.2007.00027.x). Generally, a good practice to work with a $p$ value is to regard it as another quantification, which is a random variable and has a distribution on its own. Thus, to make proper sense of it, its best combined with additional measures such as, for example, its confidence interval, an effect size, and an analysis of statistical power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6169a072232f44c682986cbe4dd0fc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 20\n",
    "alpha = 1000\n",
    "\n",
    "# calculate distribution spread to adjust plotting limits\n",
    "integrand = lambda x, N_, alpha_: x**2 * weighted_smallness_dist(x, N_, alpha_)\n",
    "var = sc.integrate.quad(integrand, -np.infty, np.infty, args=(N,alpha,))[0]\n",
    "sigma = np.sqrt(var/N)\n",
    "\n",
    "score = sample_similarity_scores(N, alpha, runs=1)\n",
    "\n",
    "# integrate over null distribution from -inf to score\n",
    "pvalue = sc.integrate.quad(similarity_score_distribution, score, np.inf, args=(N, alpha))[0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the analytical distribution\n",
    "xvalues = np.linspace(-5*sigma, 5*sigma, 100)\n",
    "ax.plot(xvalues, similarity_score_distribution(xvalues, N, alpha), 'k--',\n",
    "        label=r'null distribution $f(\\eta)$')\n",
    "\n",
    "# plot score and area ~ pvalue\n",
    "ax.axvline(score, color='r', label=r'sample score $\\eta$')\n",
    "right_to_score = np.linspace(score, 5*sigma, 100)\n",
    "ax.fill_between(right_to_score, \n",
    "                similarity_score_distribution(right_to_score, N, alpha),\n",
    "                alpha=0.6, label='~ p-value')\n",
    "ax.text(score+var/4, .2/sigma, 'p-value = {:.2}'.format(pvalue))\n",
    "\n",
    "# format plot\n",
    "ax.set_xlabel(r'similarity $\\eta$')\n",
    "ax.set_ylabel('probability density')\n",
    "ax.set_ylim((0,0.45/sigma))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='apply'></a>\n",
    "# Application to neural activity data\n",
    "\n",
    "<a id='apply-a'></a>\n",
    "## Practical considerations\n",
    "\n",
    "<a id='apply-b'></a>\n",
    "## Example data\n",
    "\n",
    "<a id='apply-c'></a>\n",
    "## Parameter scan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
